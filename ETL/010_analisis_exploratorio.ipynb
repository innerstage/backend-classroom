{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis Exploratorio de Datos\n",
    "\n",
    "A continuación, analizaremos los archivos para contestar algunas preguntas básicas. El primer paso es importar las librerías necesarias, en particular `pandas` para trabajar con la data y explorarla, y `os` para contar con distintas funciones que nos permiten explorar el directorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cuántos archivos tenemos en la carpeta?\n",
    "\n",
    "Desde nuestra ubicación, podemos abrir la carpeta con Python y hacer que nos muestre el contenido. El archivo `.DS_Store` es sólo un archivo de configuración local de Mac OS, al excluirlo podemos ver que existen 8 archivos en total. Finalmente, podemos utilizar comprensión de listas en Python para generar una lista de todos los nombres de archivo, mostraré varios ejemplos a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime una lista de los archivos en el directorio especificado.\n",
    "print(\"Todos los archivos: \", os.listdir(\"data_source\"))\n",
    "\n",
    "# Genera los nombres de archivo siguiendo un patrón sencillo.\n",
    "filenames = [\"chart{}.xlsx\".format(i) for i in range(1,9)]\n",
    "print(\"\\nPrimera forma:\\nfilenames = \", filenames)\n",
    "\n",
    "# Comprensión de lista excluyendo un valor, y luego ordenando con sorted(). \n",
    "# Si los archivos no tuviesen un patrón claro, este método es más útil.\n",
    "filenames = sorted([f for f in os.listdir(\"data_source\") if f != \".DS_Store\"])\n",
    "print(\"\\nSegunda forma:\\nfilenames = \", filenames)\n",
    "\n",
    "# Comprensión de lista tomando sólo los archivos .xlsx y ordenando con sorted().\n",
    "# Muy útil si existen distintas extensiones de archivo y quieres extraer alguna específica.\n",
    "filenames = sorted([f for f in os.listdir(\"data_source\") if \".xlsx\" in f])\n",
    "print(\"\\nTercera forma:\\nfilenames = \", filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué formato poseen? ¿Están en el mismo formato? ¿Los formatos pueden ser leídos directamente por `pandas`?\n",
    "\n",
    "En este caso, todos los archivos poseen el mismo formato (`.xlsx`), y pueden ser leídos directamente por `pandas`. Así como cualquiera de las siguientes extensiones:\n",
    "\n",
    "\n",
    "\n",
    "| Format Type | Data Description | Reader | Writer |\n",
    "|:---|:---|:---|:---|\n",
    "| text | CSV | read_csv | to_csv |\n",
    "| text | Fixed-Width Text File | read_fwf |  |\n",
    "| text | JSON | read_json | to_json |\n",
    "| text | HTML | read_html | to_html |\n",
    "| text | Local clipboard | read_clipboard | to_clipboard |\n",
    "| ---  | MS Excel | read_excel | to_excel |\n",
    "| binary | OpenDocument | read_excel |  |\n",
    "| binary | HDF5 Format | read_hdf | to_hdf |\n",
    "| binary | Feather Format | read_feather | to_feather |\n",
    "| binary | Parquet Format | read_parquet | to_parquet |\n",
    "| binary | ORC Format | read_orc |  |\n",
    "| binary | Msgpack | read_msgpack | to_msgpack |\n",
    "| binary | Stata | read_stata | to_stata |\n",
    "| binary | SAS | read_sas |  |\n",
    "| binary | SPSS | read_spss |  |\n",
    "| binary | Python Pickle Format | read_pickle | to_pickle |\n",
    "| SQL | SQL | read_sql | to_sql |\n",
    "| SQL | Google BigQuery | read_gbq | to_gbq |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cuántas filas y columnas posee cada archivo?\n",
    "\n",
    "Para analizar cada archivo, hacemos uso de la lista `filenames` que creamos previamente, y podemos imprimir algunas filas e información de cada uno. Ahora tenemos una idea sobre la estructura de cada archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in filenames: # f tomará el valor de cada nombre de archivo.\n",
    "    print(\"\\nARCHIVO: \", f) # Imprimimos el nombre del archivo para referencia.\n",
    "    df = pd.read_excel(\"data_source/\" + f) # Debemos agregar \"data_source/\" para que busque el archivo en la carpeta.\n",
    "    print(\"\\n\", df.head(), \"\\n\")\n",
    "    print(\"\\n\", df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Tenemos valores nulos (`NaN`) en ciertas columnas? ¿Cuál es la mejor estrategia para este conjunto de datos?\n",
    "\n",
    "En este caso, vamos a analizar los archivos uno por uno, testeando para encontrar valores nulos o inconsistencias en las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in filenames:\n",
    "    print(\"\\nARCHIVO: \", f)\n",
    "    df = pd.read_excel(\"data_source/\" + f)\n",
    "    print(df.isnull().any()) # Imprimimos en pantalla qué columnas poseen valores nulos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También es posible revisar los valores únicos de cada columna en cada DataFrame generado. El proceso puede ser tedioso y puede omitirse, pero se debe volver a él si es que hay errores en el futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in filenames:\n",
    "    print(\"\\nARCHIVO: \", f)\n",
    "    df = pd.read_excel(\"data_source/\" + f)\n",
    "    for c in df.keys():\n",
    "        print(\"\\nCOLUMNA: \", c)\n",
    "        print(df[c].value_counts(dropna=False)) \n",
    "\n",
    "# Imprimimos en pantalla los valores posibles en cada columna, se agrega dropna=False para poder revisar si hay NaNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontramos que ninguno de los archivos posee valores nulos y logramos entender los aspectos más importantes de la estructura en los archivos. En la siguiente etapa hablaremos de Tidy Data y cómo lograr unir estos archivos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
